{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**OBJECTIVE** : Optimize the placement of street teams at subway station entrances, in order to collect email addresses for the client (WTWY)’s annual gala. The goal is to maximize the number of emails collected from people who would be interested either in attending the gala or in contributing to the cause (“target population”)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DATA** : Our primary source of data is MTA subway data. This gives us information on the flow of people through various stations. This workbook demonstrates how we acquired, processed and scrubbed the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import datetime\n",
    "import os\n",
    "import pandas as pd\n",
    "import sys\n",
    "from collections import defaultdict\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identify MTA data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The name and location of MTA turnstile files. Files are published once\n",
    "# per week on Saturday. For description of file contents, see:\n",
    "# http://web.mta.info/developers/resources/nyct/turnstile/ts_Field_Description.txt\n",
    "\n",
    "MTA_FILE_FMT = 'http://web.mta.info/developers/data/nyct/turnstile/turnstile_%y%m%d.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_urls(n_weeks, start_date=None):\n",
    "    \"\"\"Identify MTA turnstile files in a specific interval\n",
    "    \n",
    "    n_weeks: interval duration, in weeks\n",
    "    start_date: interval start as datetime, if None function uses today() minus n_weeks \n",
    "    \n",
    "    Returns chronological list of URLs \n",
    "    \"\"\"\n",
    "    \n",
    "    ONE_WEEK = datetime.timedelta(days=7)\n",
    "\n",
    "    # If no start specified, use n_weeks before today\n",
    "\n",
    "    if None == start_date:\n",
    "        start_date = datetime.date.today() - n_weeks * ONE_WEEK\n",
    "        \n",
    "    # Identify first Saturday on/after specified date\n",
    "\n",
    "    first_saturday = start_date + datetime.timedelta(days=5 - start_date.weekday())\n",
    "    \n",
    "    # Compose list of requested dates\n",
    "    \n",
    "    dates = [first_saturday + ONE_WEEK * i for i in range(n_weeks)]\n",
    "    \n",
    "    # Return file names sorted chronologically\n",
    "    \n",
    "    return [d.strftime(MTA_FILE_FMT) for d in dates]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://web.mta.info/developers/data/nyct/turnstile/turnstile_160604.txt\n",
      "http://web.mta.info/developers/data/nyct/turnstile/turnstile_160611.txt\n",
      "http://web.mta.info/developers/data/nyct/turnstile/turnstile_160618.txt\n",
      "http://web.mta.info/developers/data/nyct/turnstile/turnstile_160625.txt\n"
     ]
    }
   ],
   "source": [
    "# Test it\n",
    "for f in get_urls(4, datetime.datetime(2016, 6, 1)):\n",
    "    print f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get data for Spring 2016 (as Gala is scheduled for the beginning of summer, so assume teams will\n",
    "be deployed in mid- / late- Spring)\n",
    "\n",
    "Convert the raw data to something more useful:\n",
    "- Collapse the rows into a dictionary that maps each STATION to the aggregate entry/exit data for all turnstiles \n",
    "- Remove invalid values and outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data(n_weeks=4, start_date=None):\n",
    "    \"\"\"Load MTA turnstile data for a specific interval\n",
    "    \n",
    "    start_date: interval start\n",
    "    n_weeks: interval duration, in weeks\n",
    "    \n",
    "    Returns date frame\n",
    "    \"\"\"\n",
    "    # Get files to read\n",
    "\n",
    "    frames = []\n",
    "    for url in get_urls(n_weeks, start_date):\n",
    "    \n",
    "        # If the file exists locally, use it\n",
    "        name = os.path.basename(url)\n",
    "        if os.path.exists(name):\n",
    "            print 'reading local:', name\n",
    "            frames.append(pd.read_csv(name))\n",
    "        else:            \n",
    "            # Otherwise, download it ...\n",
    "            print 'reading url:', url\n",
    "            frames.append(pd.read_csv(url))\n",
    "            # ...  and save a copy for the next time\n",
    "            frames[-1].to_csv(name, index=False)\n",
    "\n",
    "    return pd.concat(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_dict(df):\n",
    "    \"\"\"Convert turnstile data frame to dictionary that maps each STATION\n",
    "    to a list of { DATE : [TIME, ENTRIES, EXITS], ... } values\n",
    "\n",
    "    df: Pandas data frame with raw turnstile data\n",
    "    \"\"\"\n",
    "\n",
    "    # Build dictionary\n",
    "\n",
    "    d = OrderedDict()\n",
    "    for row in df.values:\n",
    "        key = tuple(row[:4]) # this is (C/A, UNIT, SCP, STATION)\n",
    "        date, time, entries, exits = (row[6], row[7], row[9], row[10])\n",
    "        dt = datetime.datetime.strptime(date + time, '%m/%d/%Y%H:%M:%S')\n",
    "        # Turnstile data is by the hour, only accept data that fits the\n",
    "        # expected pattern\n",
    "        if 0 == dt.minute + dt.second:\n",
    "            d.setdefault(key, []).append([dt, entries, exits])\n",
    "\n",
    "    # Here 'd' is:\n",
    "    # { (C/A, UNIT, SCP, STATION) : [[DATE-TIME, ENTRIES, EXITS], ...]}\n",
    "\n",
    "    # Convert cumulative counts to deltas\n",
    "    \n",
    "    for key, counts in d.items():\n",
    "        for i in range(len(counts) - 1, 0, -1):\n",
    "            dt1, entries1, exits1 = tuple(counts[i])\n",
    "            dt2, entries2, exits2 = tuple(counts[i-1])\n",
    "            # Turnstile data comes in 4-hour intervals. To get the deltas,\n",
    "            # subtract each count from the preceeding interval. Use the\n",
    "            # asbolute value because some turnstiles run backwards. When \n",
    "            # there's no preceeding interval, zero the count.\n",
    "            if dt1 == dt2 + datetime.timedelta(hours=4):\n",
    "                counts[i][1] = abs(entries1 - entries2) # handle negative deltas\n",
    "                counts[i][2] = abs(exits1 - exits2)\n",
    "            else:\n",
    "                counts[i][1] = 0\n",
    "                counts[i][2] = 0\n",
    "        counts.pop(0) # Discard first element: there's nothing preceding it.\n",
    "\n",
    "    # Here 'd' is:\n",
    "    # { (C/A, UNIT, SCP, STATION) : [[DATE-TIME, DELTA-ENTRIES, DELTA-EXITS], ...]}\n",
    "\n",
    "    # Exclude large outliers from dictonary\n",
    "\n",
    "    cleaned = OrderedDict()\n",
    "    for key, counts in d.items():\n",
    "        for vals in counts:\n",
    "            entries, exits = (vals[1], vals[2])\n",
    "            if entries < 50000 and exits < 50000:\n",
    "                cleaned.setdefault(key, []).append(vals)\n",
    "\n",
    "    # Aggregate the deltas by STATION and DATE-TIME\n",
    "\n",
    "    agg = OrderedDict()\n",
    "    for key, counts in cleaned.items():\n",
    "        station = key[3]\n",
    "        for vals in counts:\n",
    "            dt = vals[0]\n",
    "            agg.setdefault((station, dt), []).append(vals[1:])\n",
    "\n",
    "    # Here 'agg' is:\n",
    "    # { (STATION, DATE-TIME) : [[DELTA-ENTRIES, DELTA-EXITS], ...]}\n",
    "\n",
    "    d = OrderedDict()\n",
    "    for key, counts in agg.items():\n",
    "        station, dt = key\n",
    "        entries, exits = zip(*counts)\n",
    "        d.setdefault(station, []).append([dt.date(), dt.weekday(), dt.hour, sum(entries), sum(exits)])\n",
    "    \n",
    "    # Here 'd' is:\n",
    "    # { STATION : [DATE, WEEKDAY (0=Monday), HOUR (0-23), SUM(DELTA-ENTRIES), SUM(DELTA-EXITS)] }\n",
    "\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def dataframe_from_dict(d):\n",
    "    \"\"\"Convert turnstile data dictionary to data frame.\n",
    "\n",
    "    d: dictionary like { STATION : [[DATE, DAY, TIME, ENTRIES, EXITS], ...] }\n",
    "    \"\"\"\n",
    "    flattened = []\n",
    "    for station, tally in d.items():\n",
    "        for data in tally:\n",
    "            flattened.append([station] + data)\n",
    "    return pd.DataFrame(flattened, columns=['STATION', 'DATE', 'DAY', 'TIME', 'ENTRIES', 'EXITS'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "start_week = datetime.date(2016, 3, 15)\n",
    "week_count = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading cached: mta-spring2016-rev01.csv\n"
     ]
    }
   ],
   "source": [
    "# If the data file already exists, use it. Otherwise, create it.\n",
    "filename = 'mta-spring2016-rev01.csv'\n",
    "if os.path.exists(filename):\n",
    "    print 'reading cached:', filename\n",
    "    df = pd.read_csv(filename)\n",
    "else:\n",
    "    print 'creating:', filename\n",
    "    raw_df = load_data(week_count, start_week)\n",
    "    # Convert the data\n",
    "    d = make_dict(raw_df)\n",
    "    # Save the results\n",
    "    df = dataframe_from_dict(d)\n",
    "    df.to_csv(filename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 195019 entries, 0 to 195018\n",
      "Data columns (total 6 columns):\n",
      "STATION    195019 non-null object\n",
      "DATE       195019 non-null object\n",
      "DAY        195019 non-null int64\n",
      "TIME       195019 non-null int64\n",
      "ENTRIES    195019 non-null int64\n",
      "EXITS      195019 non-null int64\n",
      "dtypes: int64(4), object(2)\n",
      "memory usage: 8.9+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
